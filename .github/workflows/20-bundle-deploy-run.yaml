name: 20 - Deploy & Run (DAB)

on:
  workflow_dispatch:
    inputs:
      cluster_spark_version:
        description: "DBR LTS (ex.: 14.3.x-scala2.12)"
        required: false
        default: "14.3.x-scala2.12"
      node_type_id:
        description: "SKU (ex.: Standard_F4)"
        required: false
        default: "Standard_F4"
  workflow_run:
    workflows:
      - "10 - Upload arquivo para RAW"
    types: [completed]

permissions:
  contents: read
  id-token: write

jobs:
  deploy_and_run:
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    env:
      KV_NAME: akv-medalforge-rbac-core

      # Vars que alimentam o databricks.yml (prefixo BUNDLE_VAR_)
      BUNDLE_VAR_azure_client_id:  ${{ secrets.ARM_CLIENT_ID }}    # SPN bootstrap
      BUNDLE_VAR_azure_tenant_id:  ${{ secrets.ARM_TENANT_ID }}

      # Cluster params (inputs -> vars do bundle)
      BUNDLE_VAR_cluster_spark_version: ${{ github.event.inputs.cluster_spark_version || '14.3.x-scala2.12' }}
      BUNDLE_VAR_node_type_id:          ${{ github.event.inputs.node_type_id || 'Standard_F4' }}

    steps:
      - uses: actions/checkout@v4

      # 0) Login como SPN bootstrap (OIDC)
      - name: Azure login (SPN bootstrap)
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      # 1) SPN dinâmica (client_id) a partir do Key Vault -> run_as do job no bundle
      - name: Fetch dynamic SPN (client_id) from KV
        run: |
          set -euo pipefail
          SPN_ID=$(az keyvault secret show --vault-name "$KV_NAME" --name spn-client-id --query value -o tsv)
          echo "BUNDLE_VAR_dynamic_spn_client_id=$SPN_ID" >> $GITHUB_ENV

      # 2) Ler workspace_id direto do tfstate remoto e exportar para o DAB
      - name: Read workspace_id from remote tfstate
        env:
          SA_NAME: stmedalforgestate      
          CONTAINER: tfstate
          STATE_BLOB: dbx.tfstate         
        run: |
          set -euo pipefail
          sudo apt-get update -y && sudo apt-get install -y jq
          az storage blob download \
            --auth-mode login \
            --account-name "$SA_NAME" \
            --container-name "$CONTAINER" \
            --name "$STATE_BLOB" \
            --file state.json
          WORKSPACE_ID=$(jq -r '.outputs.workspace_id.value' state.json)
          if [ -z "$WORKSPACE_ID" ] || [ "$WORKSPACE_ID" = "null" ]; then
            echo "workspace_id não encontrado no tfstate ($CONTAINER/$STATE_BLOB)"
            cat state.json || true
            exit 1
          fi
          echo "BUNDLE_VAR_azure_workspace_resource_id=$WORKSPACE_ID" >> $GITHUB_ENV
          echo "Resolved workspace_id: $WORKSPACE_ID"

      # 3) Instalar Databricks CLI (bundles) e fazer deploy/run
      - name: Install Databricks CLI (bundles)
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
          databricks -v

      - name: DAB validate
        run: databricks bundle validate -t dev

      - name: DAB deploy
        run: databricks bundle deploy -t dev

      - name: DAB run job
        run: databricks bundle run -t dev ingest_bronze --no-ui
