name: 20 - Deploy & Run (DAB)

on:
  workflow_dispatch:
    inputs:
      cluster_spark_version:
        description: "DBR LTS (ex.: 14.3.x-scala2.12)"
        required: false
        default: "14.3.x-scala2.12"
      node_type_id:
        description: "SKU (ex.: Standard_F4)"
        required: false
        default: "Standard_F4"
  workflow_run:
    workflows:
      - "10 - Upload arquivo para RAW"
    types: [completed]

permissions:
  contents: read
  id-token: write

jobs:
  tests:
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # Python 
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      # Java (PySpark precisa de JDK/JRE)
      - name: Set up Java (for PySpark)
        uses: actions/setup-java@v4
        with:
          distribution: "temurin"
          java-version: "11"

      # Cache do pip
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # Instala dependências
      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
      
      - name: Set PYTHONPATH (repo root + bronze + silver)
        run: |
          echo "PYTHONPATH=${GITHUB_WORKSPACE}:${GITHUB_WORKSPACE}/bronze:${GITHUB_WORKSPACE}/silver:${PYTHONPATH}" >> $GITHUB_ENV

      # Executa testes
      - name: Run tests (unit + integration)
        run: |
          set -euo pipefail
          pytest -q \
            --cov=. --cov-report=term-missing \
            tests/unit tests/integration

  deploy_and_run:
    needs: tests
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest

    env:
      KV_NAME: akv-medalforge-rbac-core
      # Vars do bundle (as ${var.*} do YAML)
      BUNDLE_VAR_cluster_spark_version: ${{ github.event.inputs.cluster_spark_version || '14.3.x-scala2.12' }}
      BUNDLE_VAR_node_type_id:          ${{ github.event.inputs.node_type_id || 'Standard_F4' }}

    steps:
      - uses: actions/checkout@v4

      # 0) Login como SPN bootstrap (OIDC) — só para acessar o KV
      - name: Azure login (SPN bootstrap)
        uses: azure/login@v2
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      # Export para a unifed auth do Databricks via OIDC (GitHub → Azure)
      - name: Export env for Databricks auth (OIDC)
        run: |
          echo "ARM_CLIENT_ID=${{ secrets.ARM_CLIENT_ID }}" >> $GITHUB_ENV
          echo "ARM_TENANT_ID=${{ secrets.ARM_TENANT_ID }}" >> $GITHUB_ENV
          echo "ARM_USE_OIDC=true" >> $GITHUB_ENV

      # 1) Buscar client_id/secret da SPN dinâmica no Key Vault (para ler o tfstate)
      - name: Fetch dynamic SPN creds from KV
        run: |
          set -euo pipefail
          SPN_ID=$(az keyvault secret show --vault-name "$KV_NAME" --name spn-client-id --query value -o tsv)
          SPN_SECRET=$(az keyvault secret show --vault-name "$KV_NAME" --name spn-client-secret --query value -o tsv)
          TENANT_ID=${{ secrets.ARM_TENANT_ID }}

          echo "::add-mask::$SPN_SECRET"

          echo "DYN_SPN_ID=$SPN_ID"         >> $GITHUB_ENV
          echo "DYN_SPN_SECRET=$SPN_SECRET" >> $GITHUB_ENV
          echo "DYN_TENANT_ID=$TENANT_ID"   >> $GITHUB_ENV
          # run_as do job no bundle:
          echo "BUNDLE_VAR_dynamic_spn_client_id=$SPN_ID" >> $GITHUB_ENV

      - name: Export SPN envs for monitoring (Azure Table)
        run: |
          echo "BUNDLE_VAR_dynamic_spn_client_id=$SPN_ID" >> $GITHUB_ENV
          echo "BUNDLE_VAR_tenant_id=${{ secrets.ARM_TENANT_ID }}" >> $GITHUB_ENV
          echo "BUNDLE_VAR_client_secret=$SPN_SECRET" >> $GITHUB_ENV
          echo "MON_TABLE_ACCOUNT=medalforgestorage" >> $GITHUB_ENV
          echo "MON_TABLE_NAME=pipeline_runs" >> $GITHUB_ENV
          echo "ENV=dev" >> $GITHUB_ENV

      # 2) Loga como SPN dinâmica e lê workspace_id do tfstate -> DATABRICKS_AZURE_RESOURCE_ID
      - name: Read workspace_id from remote tfstate (dynamic SPN)
        env:
          SA_NAME: stmedalforgestate
          CONTAINER: tfstate
          STATE_BLOB: dbx.tfstate
        run: |
          set -euo pipefail
          sudo apt-get update -y && sudo apt-get install -y jq

          # login com SPN dinâmica (tem RBAC Storage Blob Data Contributor)
          az login --service-principal \
            --username "$DYN_SPN_ID" \
            --tenant   "${{ secrets.ARM_TENANT_ID }}" \
            --password="$DYN_SPN_SECRET" >/dev/null

          # baixa o tfstate
          az storage blob download \
            --auth-mode login \
            --account-name "$SA_NAME" \
            --container-name "$CONTAINER" \
            --name "$STATE_BLOB" \
            --file state.json >/dev/null

          WORKSPACE_ID=$(jq -r '.outputs.workspace_id.value' state.json)
          if [ -z "$WORKSPACE_ID" ] || [ "$WORKSPACE_ID" = "null" ]; then
            echo "workspace_id não encontrado no tfstate ($CONTAINER/$STATE_BLOB)"
            cat state.json || true
            exit 1
          fi

          echo "DATABRICKS_AZURE_RESOURCE_ID=$WORKSPACE_ID" >> $GITHUB_ENV
          echo "Resolved workspace_id: $WORKSPACE_ID"

      # 3) Instalar Databricks CLI (bundles) e validar/deploy/run
      - name: Install Databricks CLI (bundles)
        run: |
          curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
          databricks -v

      # inspeciona o ambiente de auth que o CLI enxerga
      - name: Debug auth env
        run: databricks auth env || true

      - name: DAB validate
        run: databricks bundle validate -t dev 

      - name: DAB deploy
        run: databricks bundle deploy -t dev 

      - name: DAB run job (aguardando conclusão)
        run: databricks bundle run -t dev one_data_proccess
