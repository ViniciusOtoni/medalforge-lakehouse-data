# One Data ‚Äî Documenta√ß√£o T√©cnica

O reposit√≥rio "One Data" √© um dos componentes da solu√ß√£o Data Master (F1rst Santander). Solu√ß√£o proposta e desenvolvida por [Vinicius Otoni](http://linkedin.com/in/vinicius-otoni-b330b3295/)

> ‚ö†Ô∏è Antes de prosseguir, leia o reposit√≥rio de infraestrutura **[Sunny Data](https://github.com/ViniciusOtoni/sunny-data)**. Todo o provisionamento (Azure + Terraform + GitHub Actions) utilizado aqui depende dele.

---

## Conte√∫dos do Reposit√≥rio


1. [Vis√£o Inicial](#1-vis√£o-inicial)  
   * [Objetivo](#11-objetivo)  
   * [Idea√ß√£o do Projeto](#12-idea√ß√£o-do-projeto)  
   * [Intera√ß√µes do usu√°rio](#13-intera√ß√µes-do-usu√°rio)

2. [Arquitetura T√©cnica](#2-arquitetura-t√©cnica)  
   * [Bronze (Ingest√£o)](#21-bronze-ingest√£o)  
   * [Silver (ETL + Qualidade)](#22-silver-etl--qualidade)

3. [Contratos de Dados](#3-contratos-de-dados)  
   * [Contrato Bronze (exemplo real)](#31-bronze)  
   * [Contrato Silver (exemplo real)](#32-silver)

4. [Execu√ß√£o (GitHub Actions + DAB)](#4-execu√ß√£o-github-actions--dab)  
   * [Workflows](#41-workflows)  
   * [Observa√ß√µes sobre o Actions](#42-observa√ß√µes-sobre-a-implementa√ß√£o-do-actions)  
   * [Databricks Asset Bundle](#43-databricks-asset-bundle)

5. [Comportamento e Limita√ß√µes](#5-comportamento-e-limita√ß√µes)  
   * [Bronze](#51-bronze)  
   * [Silver](#52-silver)  
   * [Customs (m√©todos customizados)](#53-customs-m√©todos-customizados)

6. [DQX ‚Äî Framework de Qualidade de Dados](#6-dqx--framework-de-qualidade-de-dados)

7. [Framework de ETL (Silver) do One Data](#7-framework-de-etl-silver-do-one-data)

8. [Monitoramento](#8-monitoramento)

9. [Setup](#9-pr√©-requisitos-setup)

10. [Melhorias](#10-melhorias)

11. [Refer√™ncias](#11-refer√™ncias)

---

## 1. Vis√£o Inicial

Esta documenta√ß√£o descreve a arquitetura, fluxos, contratos e mecanismos de execu√ß√£o do One Data, cobrindo a experi√™ncia do usu√°rio e o funcionamento interno do c√≥digo (Bronze e Silver, incluindo m√©todos custom). O objetivo √© registrar funcionalidades, limita√ß√µes conhecidas e pontos de extens√£o, com exemplos pr√°ticos.

### 1.1 Objetivo

O **One Data** democratiza o acesso inicial ao lakehouse, atuando em duas camadas fundamentais:

- **Bronze** ‚Üí ingest√£o bruta governada, com rastreabilidade e padroniza√ß√£o m√≠nima.  
- **Silver** ‚Üí padroniza√ß√£o, enriquecimento e qualidade (DQX), preparando dados para consumo confi√°vel.

A proposta √© reduzir a depend√™ncia de alto conhecimento t√©cnico e incentivar fluxos **autom√°ticos**, **padronizados** e **audit√°veis**.

### 1.2 Idea√ß√£o do Projeto

<p align="left">
  <img src="./assets/images/fluxo-ideacao-data.png" alt="Fluxo geral" width="100%" style="border-radius: 1%;"/>
</p>

### 1.3 Intera√ß√µes do usu√°rio

- **Bronze**: definir contrato **JSON** (schema/parti√ß√µes/op√ß√µes de leitura) e subir arquivo para RAW.  
- **Silver**: definir contrato **YAML** (source/target, DQX, ETL, quarentena, customs).  
- **Customs**: opcionalmente declarar m√©todos customizados no YAML apontando para fun√ß√µes Python dispon√≠veis no workspace.

---

## 2. Arquitetura T√©cnica

O fluxo do One Data inicia-se com o preenchimento do [Data Contract](#3-data-contract), documento respons√°vel por descrever a estrutura e as regras esperadas do dataset. A partir desse contrato, a ingest√£o √© realizada automaticamente por meio do Auto Loader, que garante escalabilidade e efici√™ncia na chegada dos dados.

Em seguida, os dados passam por uma Engine interna, apoiada pelo framework [DQX](#6-dqx--framework-de-qualidade-de-dados), respons√°vel por aplicar health checks e valida√ß√µes de qualidade, assegurando consist√™ncia, confiabilidade e ader√™ncia √†s regras de governan√ßa definidas.

### 2.1 Bronze (Ingest√£o)

Bronze Camada destinada √† ingest√£o bruta dos dados, preservando o formato original e mantendo rastreabilidade completa da origem.

<p align="left">
  <img src="./assets/images/fluxo-ingestao.png" alt="Fluxo Bronze" width="100%" style="border-radius: 1%;"/>
</p>

* **Auto Loader** em modo `availableNow` (D‚Äë1).
* **Schema** aplicado a partir do contrato; registros fora do schema v√£o para `badRecordsPath` no checkpoint.
* Escrita **Delta/UC** com cria√ß√£o idempotente de **tabela externa** (LOCATION + PARTITIONED BY + coment√°rios de coluna).
* Colunas de auditoria: `ingestion_ts` (timestamp) e `ingestion_date` (date).

### 2.2 Silver (ETL + Qualidade)

Silver Camada respons√°vel pela padroniza√ß√£o e enriquecimento dos dados, aplicando as regras do Data Contract e valida√ß√µes do DQX. Aqui os dados tornam-se confi√°veis e prontos para an√°lises mais avan√ßadas ou consumo por √°reas de neg√≥cio.

<p align="left">
  <img src="./assets/images/fluxo-silver.png" alt="Fluxo Silver" width="100%" style="border-radius: 1%;"/>
</p>

* **DQX**: checks declarados no contrato separando **v√°lidos** e **quarentena**.
* **ETL**: sequ√™ncia **declarativa** (YAML) com steps core (ex.: `trim_columns`, `normalize_dates`, `cast_columns`, `deduplicate`) e **customs** opcionais.
* **Remedia√ß√£o**: etapa de `quarantine.remediate` (ex.: `coerce_date`, `clamp_range`, `drop_if_null`) seguida de **re-check DQX**.
* **Write**: `merge` no destino com `merge_keys`; `still_bad` vai para o sink de quarentena informado.

> Os utilit√°rios de escrita garantem cat√°logo/esquema no **Unity Catalog** e realizam `MERGE`/`APPEND` com suporte a **ZORDER BY** e **partitioning** declarados.

---

## 3. Data Contract

O One Data usa contratos versionados para descrever o dataset e as regras de qualidade.

### 3.1 Bronze

```json
{
  "version": "1.0",
  "catalog": "bronze",
  "schema": "sales_new",
  "table": "orders",
  "columns": [
    { "name": "id",          "dtype": "string",  "comment": "Order ID" },
    { "name": "created_at",  "dtype": "timestamp", "comment": "Creation time" },
    { "name": "amount",      "dtype": "decimal(18,2)", "comment": "Total amount" },
    { "name": "tags",        "dtype": "array<string>", "comment": "Labels for the order" },
    { "name": "meta",        "dtype": "struct<source:string, ts:timestamp>", "comment": "Metadata" }
  ],
  "partitions": ["created_at"],
  "source": {
    "format": "json",
    "options": { "multiline": true }
  }
}
```

**Observa√ß√µes**

* **JSON em array** (`[...]`) ‚Üí usar `multiline: true` (evita linhas ‚Äúvazias‚Äù).
* **Parti√ß√µes** n√£o podem ser de **tipos complexos**.
* Os coment√°rios das colunas s√£o propagados para o UC.

### 3.2 Silver

```yaml
version: "1.0"

source:
  bronze_table: "bronze.sales_new.orders"

target:
  catalog: "silver"
  schema:  "sales"
  table:   "sales_clean"
  write:
    mode: "merge"
    merge_keys: ["id"]
    partition_by: ["ano","mes"]
    zorder_by: ["created_at"]

dqx:
  criticality_default: "error"
  checks:
    - name: not_null_id
      criticality: error
      check:
        function: is_not_null
        arguments:
          column: "id"

    - name: not_null_created_at
      criticality: error
      check:
        function: is_not_null
        arguments:
          column: "created_at"

    - name: unique_id
      criticality: error
      check:
        function: is_unique
        arguments:
          columns: ["id"]

    - name: amount_range
      criticality: error
      check:
        function: is_in_range
        arguments:
          column: "amount"
          min_limit: 0
          max_limit: 100000

  custom:
    - name: created_at_valid_date
      criticality: error
      check:
        function: sql_expression
        arguments:
          expression: "to_date(created_at, 'yyyy-MM-dd') IS NOT NULL"
          msg: "created_at inv√°lido"

etl:
  standard:
    - method: trim_columns
      args:
        columns: ["id"]

    - method: normalize_dates
      args:
        columns: ["created_at"]
        format: "yyyy-MM-dd"
        project_ano_mes: true

    - method: cast_columns
      args:
        mapping:
          amount: "double"

    - method: deduplicate
      args:
        keys: ["id"]
        order_by: ["created_at desc"]

quarantine:
  remediate:
    - method: coerce_date
      args:
        column: "created_at"
        from_patterns:
          - "M/d/yyyy"
          - "MM/dd/yyyy"
          - "d/M/yyyy"
          - "dd/MM/yyyy"
          - "yyyy-MM-dd"
          - "MM-dd-yyyy"
          - "M-d-yyyy"
        to_format: "yyyy-MM-dd"

    - method: clamp_range
      args:
        column: "amount"
        min: 0.0

    - method: drop_if_null
      args:
        columns: ["id"]

  sink:
    table: "monitoring.quarantine.sales_bronze_data"

customs:
  allow: true
  registry:
    - name: discount_rule
      module: onedata.silver.customs.custom_sales_rules
      method: apply_discount_if_high_value
      args_schema:
        percent: { type: number, required: true, min: 0, max: 50 }
        threshold: { type: number, required: true, min: 0 }
  use_in:
    - stage: standard
      method: discount_rule
      args:
        percent: 10
        threshold: 1000
```

**Observa√ß√µes**

* A ordem declarada no YAML impacta diretamente a execu√ß√£o: os m√©todos do bloco etl.standard (conforme o exemplo) s√£o executados na sequ√™ncia em que aparecem.

* O contrato de Silver organiza regras e transforma√ß√µes em quatro blocos principais:

   * dqx: define as verifica√ß√µes de qualidade (ex.: is_not_null, is_unique, is_in_range) e checks custom via sql_expression.

   * etl: lista os m√©todos de transforma√ß√£o padronizados (ex.: trim_columns, normalize_dates, cast_columns, deduplicate) e controla a ordem de aplica√ß√£o.

   * quarantine: concentra as remedia√ß√µes que s√≥ se aplicam aos registros reprovados nos checks (ex.: coerce_date, clamp_range, drop_if_null) e define o sink.table para descarte controlado dos irrecuper√°veis.

   * customs: habilita e registra m√©todos customizados do reposit√≥rio, com valida√ß√£o de par√¢metros e indica√ß√£o de onde ser√£o executados.

* O bloco target.write controla o mode, as merge_keys, partition_by e zorder_by ‚Äî esses campos afetam idempot√™ncia e desempenho.
---

## 4. Execu√ß√£o (GitHub Actions + DAB)

### 4.1 Workflows

* **10 - Upload arquivo para RAW**
  Faz upload de um arquivo do reposit√≥rio (`data/input/*`) para o container **raw** do ADLS.

* **20 - Deploy & Run (DAB)**

  * Build & Test da wheel (src/onedata).
  * L√™ **workspace_id** do tfstate remoto e exporta as ENVs necess√°rias (incluindo SPN din√¢mica para **run_as**).
  * DAB: bundle validate ‚Üí bundle deploy ‚Üí bundle run para acionar o Job que executa Bronze ‚Üí Silver.

### 4.2 Observa√ß√µes sobre a implementa√ß√£o do Actions

* OIDC com Azure: autentica√ß√£o federada (via azure/login) evita segredo est√°tico e mant√©m auditoria no Entra ID.

* DAG entre jobs: orquestra depend√™ncias entre build, deploy e run, garantindo ordem e reprodutibilidade.

* Seguran√ßa/Segredos: segredos sens√≠veis n√£o trafegam; s√£o lidos do Azure Key Vault (via [Sunny Data](https://github.com/ViniciusOtoni/sunny-data)).

* Reprodutibilidade: cada execu√ß√£o reflete o estado versionado do bundle e dos contratos (YAML/JSON).

* Escopo enxuto: o sync.include (DAB) limita o payload para o workspace, acelerando deploys e reduzindo drift.

### 4.3 Databricks Asset Bundle

##### Proposta do DAB no projeto

O DAB descreve artefatos (seu pacote Python em wheel) e recursos Databricks (Jobs) como configura√ß√£o version√°vel em databricks.yaml. Assim, controla o que executar, como executar e com qual identidade por ambiente (targets).

Como √© usado?

* Artefato: Python wheel constru√≠da de src/onedata.

* Recurso: Job com python_wheel_task chamando entry points (ex.: Bronze), em job clusters ef√™meros.

* Identidade: run_as.service_principal_name = SPN din√¢mica, garantindo auditoria/governan√ßa.

* Target dev: concentra par√¢metros e sync.include (somente c√≥digo/contratos necess√°rios), reduzindo upload e tempo de deploy.

##### Ciclo operacional

* databricks bundle validate ‚Üí valida vari√°veis, paths e recursos.

* databricks bundle deploy ‚Üí empacota e publica no workspace (/.bundle/<nome>/<target>/...).

* databricks bundle run -t <target> <job> ‚Üí dispara o Job do bundle para o target.

##### Vari√°veis & parametriza√ß√£o

* Declaradas em variables: e injetadas por ENV com DATABRICKS_BUNDLE_VAR_<nome> (ex.: dynamic_spn_client_id).

* targets: permitem diferenciar ambientes (ex.: DBR version, node_type_id, sync.include).

##### Benef√≠cios pr√°ticos

* Menos drift: evita configura√ß√µes manuais no workspace.

* Reprodut√≠vel: mesmo bundle ‚Üí mesmo resultado por target.

* Governan√ßa: execu√ß√£o com SPN din√¢mica (run_as).

* Deploys r√°pidos: sync.include enxuto e job clusters ef√™meros.

**Aten√ß√µes!**

* A wheel precisa de empacotamento v√°lido (ex.: pyproject.toml/setup.cfg).

* Somente m√≥dulos dentro de sync.include ficam dispon√≠veis no cluster (evite import fora desses caminhos).

* Trocas de cluster_spark_version/node_type_id impactam custo e compatibilidade.

---

## 5. Comportamento e Limita√ß√µes

### 5.1 Bronze

* **JSON (array)**: requer `multiline: true` no contrato.
* **Parti√ß√µes**: n√£o suportam tipos complexos e **devem existir no schema**.
* **Backlog**: existe suporte a `includeExistingFiles` (por execu√ß√£o); checkpoints s√£o isol√°veis por `reprocess_label`.
* **Schema enforcement**: registros fora do schema s√£o enviados a `badRecordsPath` (√∫til para debug).

### 5.2 Silver

* **Ordem importa**: steps de `etl.standard` rodam **na ordem declarada**. Ex.: normalizar datas **antes** dos checks reduz falsos positivos; `deduplicate` depois de `cast_columns` evita m√∫ltiplas vers√µes de uma mesma chave de merge.
* **Merge**: `merge_keys` devem estar presentes e, ap√≥s `deduplicate`, serem √∫nicas; caso contr√°rio, o MERGE pode falhar ou sobrescrever linhas indevidamente.
* **Quarentena**: o `sink.table` √© respeitado, por√©m a escrita √© feita como **tabela externa** usando a base configurada por `SILVER_EXTERNAL_BASE`. Ou seja, mesmo com FQN `monitoring.*`, o **LOCATION** f√≠sico fica sob a base da Silver. Se desejar bases f√≠sicas distintas por cat√°logo, ser√° necess√°rio estender o c√≥digo.
* **Z-Order e Particionamento**: aplicados conforme contrato; mau uso pode degradar performance (ex.: `zorder_by` em coluna de baixa cardinalidade).
* **Monitoria**: `PipelineRunLogger` √© **soft‚Äëdependency**; sem libs/ENVs da Azure o logger vira no‚Äëop (a execu√ß√£o do pipeline n√£o para).

### 5.3 Customs (m√©todos customizados)

* **Registro e uso**: `customs.registry` define *onde* (m√≥dulo/m√©todo) e o `args_schema` de valida√ß√£o; `customs.use_in` escolhe **em qual est√°gio** aplicar.
* **Assinatura esperada**: `fn(df: DataFrame, **kwargs) -> DataFrame`.
* **Seguran√ßa**: o loader suporta restri√ß√£o por prefixos de m√≥dulo e exige marca√ß√£o via decorator `@custom` (ver `src/onedata/silver/customs/sdk.py`).
* **Flags de execu√ß√£o**: o runner exp√µe flags/ENVs para modo estrito e prefixos permitidos (ver `src/onedata/silver/application/settings.py`).

Exemplo (j√° inclu√≠do no repo):

```python
# src/onedata/silver/customs/custom_sales_rules.py
from pyspark.sql import DataFrame, functions as F
from onedata.silver.customs.sdk import custom

@custom
def apply_discount_if_high_value(df: DataFrame, percent: float, threshold: float) -> DataFrame:
    # Se amount >= threshold, aplica desconto e marca discount_applied
    if percent < 0 or percent > 50:
        raise ValueError("percent fora do intervalo [0,50]")
    if "amount" not in df.columns:
        raise ValueError("coluna 'amount' ausente")
    return (
        df
        .withColumn("discount_applied", (F.col("amount") >= F.lit(threshold)))
        .withColumn("amount", F.when(F.col("discount_applied"), F.col("amount") * (1 - percent/100)).otherwise(F.col("amount")))
    )
```

---

# 6. DQX ‚Äî Framework de Qualidade de Dados

O DQX (Databricks Labs) √© um framework de qualidade de dados para PySpark que permite definir, validar e monitorar regras de qualidade sobre DataFrames (batch ou streaming). Ele fornece fun√ß√µes prontas (ex.: is_not_null, is_unique, is_in_range) e possibilita checks customizados (ex.: express√µes SQL), retornando detalhes sobre por que cada check falhou e facilitando estrat√©gias de quarentena e remedia√ß√£o.

### Como o One Data utiliza?

* O DQX √© acionado na Silver (bloco dqx do contrato), separando v√°lidos de inv√°lidos antes do merge.

* Checks custom (ex.: sql_expression) cobrem regras espec√≠ficas de neg√≥cio.

* Os resultados orientam a etapa quarantine.remediate, onde os dados problem√°ticos s√£o corrigidos e revalidados.

**Melhor conhecimento sobre o DQX**

- GitHub (Databricks Labs): https://github.com/databrickslabs/dqx
- Docs: https://databrickslabs.github.io/dqx/

> Observa√ß√£o: DQX √© um projeto Labs (open-source, evolu√ß√£o cont√≠nua). Consulte o reposit√≥rio para evolu√ß√£o e breaking changes.

---

# 7. Framework de ETL (Silver) do One Data

O framework de ETL da Silver √© declarativo, dirigido por YAML e constru√≠do para ser gen√©rico (padroniza√ß√£o) com pontos de extens√£o controlados (customs). Ele n√£o cria features al√©m do que est√° no repo ‚Äî atua exatamente conforme o contrato fornecido.

### Estrutura (espelhando o YAML)

* source: origem FQN (ex.: bronze.sales_new.orders).

* target: destino FQN + estrat√©gia de escrita (mode, merge_keys, partition_by, zorder_by).

* dqx: regras de qualidade (nativas + sql_expression).

* etl.standard: sequ√™ncia de m√©todos built-in (ex.: trim_columns, normalize_dates, cast_columns, deduplicate).

* quarantine: remedia√ß√£o dos reprovados (ex.: coerce_date, clamp_range, drop_if_null) e sink.table para descarte controlado.

* customs: registro e aplica√ß√£o de m√©todos custom com args_schema e est√°gio (use_in.stage) definidos.

### Ordem dos m√©todos (comportamento real)

A ordem declarada em etl.standard √© a ordem de execu√ß√£o.

Ex.: normalize_dates antes do DQX reduz falsos negativos em datas; deduplicate ap√≥s cast_columns evita chaves de merge duplicadas por varia√ß√£o de tipo.

### Escrita e idempot√™ncia

write.mode: merge + merge_keys definem o upsert.

Ap√≥s DQX e remedia√ß√£o, somente v√°lidos v√£o para o destino; inv√°lidos persistentes seguem para o sink.table.

O utilit√°rio de escrita respeita o FQN no UC, mas fisicamente usa a base definida por [SILVER_EXTERNAL_BASE](#52-silver).

### Extensibilidade segura (customs)

* Carregamento controlado por prefixo de m√≥dulo + decorator @custom.

* Valida√ß√£o de par√¢metros via args_schema.

* Contrato manda: onde e como aplicar (sem writes dentro do m√©todo).

---

## 8. Monitoramento

O monitoramento do **One Data** garante visibilidade, auditoria e rastreabilidade de todas as execu√ß√µes do pipeline, desde a ingest√£o (Bronze) at√© a aplica√ß√£o de qualidade e transforma√ß√£o (Silver). Ele foi desenhado para funcionar em camadas:

### 8.1 N√≠vel de execu√ß√£o

- **GitHub Actions**: cada pipeline CI/CD registra logs completos da execu√ß√£o, incluindo etapas de build, deploy e run.  
- **Databricks Workflows/Jobs**: a execu√ß√£o dos *bundles* gera logs no pr√≥prio workspace (dispon√≠veis via UI ou API), contendo m√©tricas de tarefas, tempo de execu√ß√£o e poss√≠veis falhas.

### 8.2 N√≠vel de dados

- **PipelineRunLogger**: m√≥dulo interno respons√°vel por registrar m√©tricas de ingest√£o e transforma√ß√£o (ex.: n√∫mero de registros v√°lidos, inv√°lidos, enviados √† quarentena).  
  - Funciona como *soft-dependency*: se n√£o houver integra√ß√£o com Azure, os logs s√£o ignorados sem interromper a execu√ß√£o.  
  - Quando habilitado, envia registros para **Azure Table Storage**.

- **DQX**: framework utilizado na camada Silver. Al√©m de validar regras, gera relat√≥rios detalhados sobre quais registros falharam em cada *check*. Esses relat√≥rios s√£o fundamentais para auditoria e podem ser integrados com monitorias externas.

### 8.3 N√≠vel de quarentena

- Todos os dados rejeitados no DQX ou imposs√≠veis de remedia√ß√£o s√£o persistidos em **tabelas de quarentena** (ex.: `monitoring.quarantine.*`).  
- Essas tabelas permitem:  
  - Identificar tend√™ncias de falha (ex.: contratos mal configurados, fontes corrompidas).  
  - Fornecer feedback r√°pido para usu√°rios que definiram o contrato.  
  - Evitar perda de dados ‚Äî nada √© simplesmente descartado.

### 8.4 Alertas e auditoria

- **Observabilidade Azure**: integra√ß√£o opcional com **Azure Monitor / Application Insights / Log Analytics** pode ser habilitada para capturar logs de execu√ß√£o e eventos cr√≠ticos.  
- **Auditoria de Identidade**: como o bundle executa com a SPN din√¢mica (`run_as.service_principal_name`), cada a√ß√£o √© registrada no **Entra ID** e no **Databricks Audit Logs**, garantindo rastreabilidade.

### 8.5 Benef√≠cios do modelo de monitoramento

- **Transpar√™ncia**: cada etapa do pipeline pode ser auditada, desde a subida do arquivo at√© o merge final.  
- **Governan√ßa**: dados inv√°lidos nunca s√£o perdidos; sempre ficam acess√≠veis em quarentena.  
- **A√ß√£o r√°pida**: logs centralizados em Azure Monitor + m√©tricas de DQX permitem identificar erros de qualidade em tempo quase real.  
- **Escalabilidade**: o monitoramento acompanha a esteira de dados sem sobrecarregar o fluxo principal.

## 9. Pr√©-requisitos (Setup)

- Conta na Azure
- Subscri√ß√£o na Azure
- Realizar o az login

### 9.1 Cria√ßao da SPN Bootstrap:

````bash
az ad sp create-for-rbac --name "terraform-admin-spn-user" --role="Contributor" --scopes="/subscriptions/<subscriptionID>"
````

Ser√° retornado o *PASSWORD*, *TENANT* e *APPID* ap√≥s a cria√ß√£o. Esses valores, precisam ser cadastrados nas Secrets do reposit√≥rio GitHub.

- **ARM_CLIENT_SECRET** -> *PASSWORD*
- **ARM_TENANT_ID** -> *TENANT*
- **ARM_CLIENT_ID** -> *APPID*
- **ARM_SUBSCRIPTION_ID** -> *subscriptionID*

<video width="40%" height="300px" controls>
  <source src="./assets/videos/adicionar-secret.mp4" type="video/mp4">
</video>

- Recupere o *OBJECT_ID* para gravar na secret **ARM_OBJECT_ID**

````bash
az ad sp show --id <appId> --query id -o tsv  
````

- Cadastre os valores seguindo esse formato em JSON na secret **AZURE_CREDENTIALS**

````json
{
  "clientId": <appId>,
  "clientSecret": <password>,
  "tenantId":   <tenant>,
  "subscriptionId": <subscriptionId>
}
````

- Atribua a role de *User Access Administrator* para a SPN

````bash
az role assignment create \
    --assignee-object-id <SPN_OBJECT_ID> \
    --role "User Access Administrator" \
    --scope "/subscriptions/<subscriptionId>"
````

- Atribuir a SPN como Cloud Application Administrator

<video width="40%" height="300px" controls>
  <source src="./assets/videos/cloud-application.mp4" type="video/mp4">
</video>

- Adicione essas duas roles no Microsoft Graph *Directory.ReadWrite.All* e *Group.ReadWrite.All*

<video width="40%" height="300px" controls>
  <source src="./assets/videos/MicrosoftGraph.mp4" type="video/mp4">
</video>

### 9.2 Atribui√ß√£o da SPN din√¢mica como Account Admin

> Essa etapa deve ser realizada apenas quando o workflow databricks-workspace exigir valida√ß√£o no JOB **üö¶ Aguardar grant account_admin**

- Entre no console de account do Databricks (https://accounts.azuredatabricks.net/)
    Para logar, informe o seu e-mail **UPN** recuperado no Microsoft Entra ID

<video width="40%" height="300px" controls>
  <source src="./assets/videos/UPN.mp4" type="video/mp4">
</video>

- Atribua a SPN din√¢mica como Account Admin

<video width="40%" height="300px" controls>
  <source src="./assets/videos/accountAdmin.mp4" type="video/mp4">
</video>


- Delete o metastore criado por Default

<video width="40%" height="300px" controls>
  <source src="./assets/videos/metastore.mp4" type="video/mp4">
</video>

- Recupere o valor do Account ID e grave na secret do GitHub **ARM_ACCOUNT_ID**

<video width="40%" height="300px" controls>
  <source src="./assets/videos/accountID.mp4" type="video/mp4">
</video>

- Aprovar JOB para finaliza√ß√£o do Workflow

<video width="40%" height="300px" controls>
  <source src="./assets/videos/approvement.mp4" type="video/mp4">
</video>

### 9.3 Workflow de execu√ß√£o do One Data

* Execute o workflow **10 - Upload arquivo para RAW**, respons√°vel por enviar seu arquivo (ex: JSON, CSV, TXT..) para o container `raw` do SA. Ap√≥s isso, o workflow **20 - Deploy & Run (DAB)**, ser√° iniciado de forma autom√°tica, dando in√≠cio no processo de deployment do JOB Databricks e execu√ß√£o do mesmo.

--- 

## 10. Melhorias

Pontos a serem melhorados para maior **robustez** e **qualidade** do projeto.

### 10.1 Seguran√ßa e Governan√ßa

 * SCIM/Grupos de Conta: migrar cria√ß√£o de grupos para account-level via SCIM e databricks_mws_permission_assignment, evitando drift de permiss√µes.

 * Policies de Cluster/SQL Warehouse: aplicar cluster policies m√≠nimas (vers√£o DBR LTS, autoscaling, tags, init scripts restritos).

 * Cat√°logo de External Locations: validar que credenciais/external locations t√™m least privilege com READ FILES apenas onde necess√°rio.

### 10.2 Confiabilidade e Qualidade

 * Schema Evolution Controlado (Bronze): permitir cloudFiles.schemaEvolutionMode opcional (e.g. addNewColumns) com guard rails via contrato.

 * Alertas de Schema Drift: comparar contrato vs. schema observado e emitir event (Azure Monitor) quando houver diverg√™ncia.

 * DQX Enriquecido: adicionar checks de referential integrity (via join probes configur√°veis) e limites percentuais de erro por dataset.

### 10.3 Performance e Custos

 * Auto Optimize/Compaction: habilitar auto compaction e optimize writes para tabelas alvo; agendar OPTIMIZE ZORDER em colunas de alta seletividade.

 * Small Files: tuning de maxFilesPerTrigger e cloudFiles.maxBytesPerTrigger no Auto Loader para evitar small files.

 * Caching Seletivo: cache somente em steps de transforma√ß√£o pesados e reutilizados (em clusters com mem√≥ria suficiente).

### 10.4 Manutenibilidade e DX

 * Versionamento de Contratos: adotar semver + migra√ß√µes (ex.: v1 ‚Üí v1.1), validando com Pydantic e gerando changelog autom√°tico.

 * CLI do Projeto: criar onedata CLI (Typer/Click) para validar contratos, plan de execu√ß√£o e dry-run local.

### 10.5 Roadmap Databricks

 * DLT (Delta Live Tables): opcionalmente migrar a orquestra√ß√£o da Silver para DLT com expectations nativas e event log centralizado.

 * Unity Catalog - Data Discovery: padronizar TBLPROPERTIES (owner, domain, pii_level) e column comments para cat√°logos de dados.

## 11. Refer√™ncias

* [Sunny Data (Infra/Terraform)](https://github.com/ViniciusOtoni/sunny-data): 
* [Databricks](https://learn.microsoft.com/pt-br/azure/databricks/)
* [DAB Databricks Asset Bundles](https://learn.microsoft.com/pt-br/azure/databricks/dev-tools/bundles/)
* [GitHub Actions](https://github.com/features/actions?locale=pt-BR)
* [Azure Table Storage](https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview)
* [DQX Framework](https://databrickslabs.github.io/dqx/)
* [Arquitetura Medallion](https://www.databricks.com/glossary/medallion-architecture)
* [Least Privilege](https://learn.microsoft.com/en-us/entra/id-governance/scenarios/least-privileged)
* [Auto Loader](https://learn.microsoft.com/pt-br/azure/databricks/ingestion/cloud-object-storage/auto-loader/)
