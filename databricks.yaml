bundle:
  name: one-data-proccess
  databricks_cli_version: ">=0.220.0"

variables:
  dynamic_spn_client_id:
    description: "Client ID da SPN dinâmica para run_as e permissions"
  cluster_spark_version:
    description: "DBR LTS do job cluster"
    default: "14.3.x-scala2.12"
  node_type_id:
    description: "SKU do node do job cluster"
    default: "Standard_F4"

# Executa o job como a SPN dinâmica (passada via var no deploy)
run_as:
  service_principal_name: ${var.dynamic_spn_client_id}

targets:
  dev:
    default: true
    sync:
      include:
        - bronze/**
        - silver/**
        - monitoring/**   # módulo de monitoria
      exclude:
        - .git/**
        - .github/**

    resources:
      jobs:
        one_data_proccess:
          name: one_data_proccess
          permissions:
            - level: CAN_MANAGE
              service_principal_name: ${var.dynamic_spn_client_id}

          tasks:
            - task_key: bronze_ingest
              spark_python_task:
                python_file: ./bronze/main.py
                parameters:
                  - --mode
                  - validate+plan+ingest
                  - --contract_path
                  - ./contracts/dummy.json
              job_cluster_key: small_job_cluster
              libraries:
                - pypi: { package: "pydantic>=2,<3" }
              environment_variables:
                # Auth SPN dinâmica
                AZURE_CLIENT_ID: ${var.dynamic_spn_client_id}
                AZURE_TENANT_ID: ${env.AZURE_TENANT_ID}
                AZURE_CLIENT_SECRET: ${env.AZURE_CLIENT_SECRET}
                # Monitoria
                MON_TABLE_ACCOUNT: medalforgestorage
                MON_TABLE_NAME: pipeline_runs
                ENV: dev

            - task_key: silver_transform
              depends_on:
                - task_key: bronze_ingest
              spark_python_task:
                python_file: ./silver/main.py
                parameters:
                  - --contract_path
                  - ./contracts/dummy.yaml
              job_cluster_key: small_job_cluster
              libraries:
                - pypi: { package: "pydantic>=2,<3" }
                - pypi: { package: "databricks-labs-dqx==0.8.0" }
                - pypi: { package: "azure-identity>=1.16.0" }
                - pypi: { package: "azure-data-tables>=12.5.0" }
              environment_variables:
                # Auth SPN dinâmica
                AZURE_CLIENT_ID: ${var.dynamic_spn_client_id}
                AZURE_TENANT_ID: ${env.AZURE_TENANT_ID}
                AZURE_CLIENT_SECRET: ${env.AZURE_CLIENT_SECRET}
                # Monitoria
                MON_TABLE_ACCOUNT: medalforgestorage
                MON_TABLE_NAME: pipeline_runs
                ENV: dev

          job_clusters:
            - job_cluster_key: small_job_cluster
              new_cluster:
                spark_version: ${var.cluster_spark_version}
                node_type_id: ${var.node_type_id}
                autoscale:
                  min_workers: 1
                  max_workers: 2
                azure_attributes:
                  availability: SPOT_WITH_FALLBACK_AZURE
                data_security_mode: SINGLE_USER
                custom_tags:
                  project: one-data
